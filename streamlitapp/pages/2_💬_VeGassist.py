import streamlit as st
from streamlit_modal import Modal
import os
import re
import json
import pandas as pd
import numpy as np
from openai import AzureOpenAI


class VegaAssistant:
  def __init__(self, embeddings_source, system_prompt):
    os.environ["AZURE_OPENAI_DeploymentId"] = "gpt-4-turbo"
    self.client = st.session_state["client"]
    contents = []
    obj = json.load(open(embeddings_source, "r"))
    for i in range(len(obj)):
      contents.append({'content': obj[i]["content"], "embedding": obj[i]["embedding"]})
    
    self.embeddings = pd.DataFrame(contents, columns=['content', 'embedding'])
    self.system_prompt = system_prompt
    self.trace = []

  def _cosine_similarity(self, a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
  
  def _get_embedding(self, text, model="ada-002"): # model = "deployment_name"
    return self.client.embeddings.create(input = [text], model=model).data[0].embedding

  def search_docs(self, _embeddings: pd.DataFrame, user_query, top_n=4):
      embedding = self._get_embedding(
          user_query,
          model="ada-002" # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model
      )
      embeddings = _embeddings.copy()
      embeddings["similarities"] = embeddings["embedding"].apply(lambda x: self._cosine_similarity(x, embedding))

      res = (
          embeddings.sort_values("similarities", ascending=False)
          .head(top_n)
      )
      return res

  def generate_answer(self, prompt, system_prompt):
    trace = [
        {"role": "system", "content": system_prompt},
      ]
    trace.extend(st.session_state.vegassistmsg)
    res = self.client.chat.completions.create(
      model=os.getenv("AZURE_OPENAI_DeploymentId"),
      #response_format={"type": "json_object"},
      messages=trace,
      stream=True
    )
    #self.trace.append({
    #  "role": res.role,
    #  "content": res.content
    #})
    
    return res

  def chat(self, prompt):
    res = self.search_docs(self.embeddings, prompt, top_n=2)

    sys_prompt = self.system_prompt + "\n".join(res["content"])
    answer = self.generate_answer(prompt, sys_prompt)
    return answer

st.set_page_config(page_title="VÃ©Ga - VeGassist", page_icon="ğŸ“„")

st.write("# ğŸ“„ VeGassist")

system_prompt = """Objectif Principal : Tu es un assistant qui doit permettre aux utilisateurs de l'ACPR d'accÃ©der facilement aux documents normatifs relatifs Ã  la supervision des activitÃ©s des Ã©tablissements financiers et de vÃ©rifier la conformitÃ© des phrases relatives aux produits financiers aux rÃ¨glements en vigueur.
1. Recherche Documentaire :
- Si lâ€™utilisateur te demande de rechercher des documents normatifs en utilisant des mots-clÃ©s, des catÃ©gories spÃ©cifiques ou des rÃ©fÃ©rences rÃ¨glementaires, tu dois uniquement citer les documents faisant rÃ©fÃ©rence Ã  ces Ã©lÃ©ments dans une liste ordonnÃ©e.
- Si la formulation de lâ€™utilisateur est trop complexe, demande-lui segmenter ses requÃªtes
- Si lâ€™utilisateur ne parvient pas Ã  trouver ce quâ€™il cherche, donne-lui des exemples de phrases Ã  renseigner. Par exemple, donne-lui des mots clÃ©s ou des catÃ©gories. 
- Si tu lis une abrÃ©viation dans un document ou si tu lis une abrÃ©viation dans la phrase de lâ€™utilisateur et que tu ne comprends pas lâ€™abrÃ©viation, cite lâ€™abrÃ©viation et demande Ã  lâ€™utilisateur de la dÃ©finir. 
2. AccÃ¨s aux Informations :
- Tu dois extraire de maniÃ¨re prÃ©cise les informations pertinentes des documents normatifs en rÃ©ponse aux requÃªtes de l'utilisateur.
- Tu dois restituer le contexte entourant une information pour une meilleure comprÃ©hension. Fais bien attention, Ã  sÃ©parer la citation du texte normatif des informations contextuelles. Le texte normatif doit Ãªtre clairement identifiable et ne doit pas avoir Ã©tÃ© modifiÃ©. 
3. Interaction Naturelle :
- Favorise une interaction conversationnelle naturelle avec l'utilisateur en comprenant le langage courant et en fournissant des rÃ©ponses comprÃ©hensibles.
- RÃ©ponds avec un langage formel est clair. Ce que tu Ã©cris doit pouvoir Ãªtre prÃ©sentÃ© dans des rapports officiels. 
4. ConformitÃ© des Phrases aux RÃ¨glements :
Si la phrase est jugÃ©e conforme, Ã©cris Â« Aucune procÃ©dure Ã  suivre Â» dans le dossier JSON, sinon cite la prÃ©ocÃ©dure Ã  suivre. 
- Si la phrase nâ€™est pas en relation avec le point de contrÃ´le, produis un fichier JSON expliquant que la phrase nâ€™est pas en relation avec le point de contrÃ´le. 
- Si des champs du fichier JSON ne sont pas applicables au contexte, ne les inclus pas.
5. Gestion des Erreurs et AmbiguÃ¯tÃ©s :
- Tu dois gÃ©rer les situations oÃ¹ une requÃªte est ambiguÃ« ou incomplÃ¨te en demandant des clarifications.
- Si lâ€™utilisateur fait une faute de frappe, corrige-la en citant la correction et rÃ©ponds Ã  la question posÃ©e par la phrase corrigÃ©e. 
- Si tu ne connais pas la rÃ©ponse, dis-le et ne cherche pas Ã  rajouter dâ€™autres Ã©lÃ©ments Ã  part des questions pour plus de prÃ©cision.
6. Mises Ã  Jour LÃ©gales :
- Tu dois informer les utilisateurs des modifications rÃ©centes dans la lÃ©gislation financiÃ¨re en prÃ©cisant les dates. 
7. Assistance et Support :
- Fournis un support contextuel pour aider les utilisateurs Ã  formuler des requÃªtes de maniÃ¨re efficace. Attention, cela ne doit jamais modifier les textes normatifs dans tes rÃ©ponses.
- Propose des questions annexes une fois que tu as satisfait la requÃªte de lâ€™utilisateur."""
assist = VegaAssistant("dump_controlpoint_embeddings.json", system_prompt)

def resetmsg():
    st.session_state.vegassistmsg = [{"role": "assistant", "content": "_Salut !_ Comment puis-je t'aider ?"}]

if "vegassistmsg" not in st.session_state:
    resetmsg()

st.write("VeGassist est votre assistant de vÃ©rification de conformitÃ©. Possez-lui une question et il vous rÃ©pondra !")
st.columns((3,2))[1].button("ğŸ” RÃ©initialiser la conversation", use_container_width=True, on_click=resetmsg)

chat = st.container(border=True)
for message in st.session_state.vegassistmsg:
    with chat.chat_message(message["role"]):
        st.markdown(message["content"])

prompt = st.chat_input("Posez une question ici")
if prompt:
    st.session_state.vegassistmsg.append({"role": "user", "content": prompt})
    with chat.chat_message("user"):
        st.markdown(prompt)
    with chat.chat_message("assistant"):
        stream = assist.chat(prompt)
        response = st.write_stream(stream)
    st.session_state.vegassistmsg.append({"role": "assistant", "content": response})